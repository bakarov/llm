Task Description

The provided code addresses the two main components of the technical task: extracting key terms from the contract and analyzing task descriptions for compliance with these extracted terms.

![Technical Task Solution](technical_task_solution.png)

The main idea of the architecture is to create a system that efficiently handles multiple queries of task descriptions against the same contract conditions using a zero-shot learning approach with BERT. Initially, a large language model (LLM) is employed to extract key terms and constraints from the contract. This extraction is a one-time process for a given contract, and the extracted terms are stored in a structured format, such as JSON. The extraction model can be fine-tuned to the specific domain to enhance its accuracy and relevance, ensuring that the terms and conditions are well-aligned with the domain-specific language and nuances.

In the current implementation, the T5-small model was utilized for most of the experiments. The choice of T5-small was primarily driven by its efficiency and reduced computational requirements, which facilitated rapid prototyping and initial testing. While T5-small is a powerful model for many natural language processing tasks, it is not the most performant version of the T5 family, which includes larger models like T5-base, T5-large, and T5-3B. The T5-small model's relative simplicity and speed make it an excellent choice for early-stage development and experimentation. It allows us to quickly iterate and refine the system's architecture and functionality without incurring high computational costs. This efficiency is particularly valuable during the initial stages when the focus is on validating the approach and identifying potential improvements.

While larger language models can provide better performance, the inference times can be an issue for environments that require real-time alerting and rapid continuous improvement. Assuming this is the case in our task, we use a different approach for the second part, Task Descriptions Analysis. Here, we use BERT-base-uncased to embed both the extracted contract terms and the task descriptions. BERT-base-uncased provides contextual embeddings that capture the nuances of the contract language and task descriptions. This approach allows us to leverage cosine similarity to determine the compliance of each task description with the contract terms. This zero-shot approach ensures that the system can handle new and unseen task descriptions without the need for retraining, providing flexibility and adaptability. The system also incorporates a feedback mechanism that allows continuous improvement over time. This continuous learning capability ensures that the system remains relevant and accurate, providing a robust solution for contract condition extraction and task compliance analysis.

Using BERT-base-uncased in this case is justified by its lightweight nature and ease of use, as it has been proven as a strong baseline model for many standard NLP tasks. To improve performance further, models such as RoBERTa, ALBERT, S-BERT, or even more specialized versions like Legal-BERT can be considered. We can also scale the solution to other languages by incorporating multilingual versions of BERT. Just like in the case of the choice of LLM, the performance of the system here can also be drastically improved by adjusting the model's parameters, selecting different models, or even using a different type of pre-processing (notably, in this task we used a standard pipeline of lemmatization and n-grams). However, all these methods require additional experimentation and careful analysis of the data, potentially investigating different methods of data augmentation to better fine-tune the models given the limited amount of initial data. We used a fairly straightforward approach in this task just for illustrative purposes of the general architecture idea.

For the future development of this project, we can consider optimizing computational cost and efficiency by employing model distillation, implementing efficient batch processing, and optimizing the model serialization process for the target deployment environment. Given the challenges with LLM interpretability and the overall black-box nature of these models (especially compared to more straightforward tools like Logistic Regression or Nearest Neighbour Search), we can incorporate tools like Langfuse (for better prompt management and monitoring), LangChain (for integrating the model with other data sources and APIs), and AutoGen (for employing multiple agents to adapt to multiple different domains simultaneously).